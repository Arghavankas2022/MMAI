{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TumorVision\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rEqiCBo8S0hk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The script below uses CLIP, fine-tuned on a simple brain tumor MRI dataset available [here](https://huggingface.co/datasets/tanzuhuggingface/brainmri), for the task of brain tumor detection. CLIP uses both images and text, mapping them into a shared embedding space. This space is learned using contrastive learning, so that semantically related image-text pairs are close together, while unrelated pairs are far apart. The approach is inspired by the notebook presented in class."
      ],
      "metadata": {
        "id": "2S2ECP7vk047"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install the necessary libraries\n"
      ],
      "metadata": {
        "id": "SR2bZ4dmO-qq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install --upgrade transformers\n",
        "!pip install accelerate\n",
        "!pip install datasets torch pillow matplotlib scikit-learn bitsandbytes\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import CLIPModel, CLIPProcessor\n"
      ],
      "metadata": {
        "id": "W5YDar5dPCxQ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Log into hugging face"
      ],
      "metadata": {
        "id": "Q12lFeDjPCoB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "6zr4uD02PNzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, platform\n",
        "print('Torch:', torch.__version__, '| CUDA:', torch.cuda.is_available(), '| GPUs:', torch.cuda.device_count())\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU:', torch.cuda.get_device_name(0))\n",
        "print('Python:', platform.python_version())"
      ],
      "metadata": {
        "id": "fCBP34znPRlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import dataset"
      ],
      "metadata": {
        "id": "WqTZZgU4PU83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset(\"tanzuhuggingface/brainmri\")"
      ],
      "metadata": {
        "id": "2jFMSAXoPUfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds"
      ],
      "metadata": {
        "id": "ec2MyIhzPqqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split dataset to train and test"
      ],
      "metadata": {
        "id": "2TOm7DVKQN46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "X = list(ds['train']['image'])\n",
        "y = list(ds['train']['label'])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "MMnAkb51TWuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import necessary libraries and model"
      ],
      "metadata": {
        "id": "j3YKENCIaVhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 2\n",
        "LR = 1e-5\n",
        "MODEL_ID = \"openai/clip-vit-base-patch32\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "clip_model = CLIPModel.from_pretrained(MODEL_ID).to(device)\n",
        "clip_processor = CLIPProcessor.from_pretrained(MODEL_ID)\n",
        "optimizer = torch.optim.AdamW(clip_model.parameters(), lr=LR)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "8IUsDpLFaUF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add text prompts"
      ],
      "metadata": {
        "id": "RSnK2iFXaeSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_prompts = [\n",
        "    \"a brain MRI with no tumor\",\n",
        "    \"a brain MRI showing a tumor\"\n",
        "]"
      ],
      "metadata": {
        "id": "FY-cbsV-ahWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load dataset via dataloader"
      ],
      "metadata": {
        "id": "v45FhmCgaimA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BrainMRIDataset(Dataset):\n",
        "    def __init__(self, images, labels):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"image\": self.images[idx],\n",
        "            \"label\": self.labels[idx]\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    images = [item[\"image\"] for item in batch]\n",
        "    labels = torch.tensor([item[\"label\"] for item in batch], dtype=torch.long)\n",
        "    return {\"images\": images, \"labels\": labels}\n",
        "\n",
        "train_dataset = BrainMRIDataset(X_train, y_train)\n",
        "test_dataset  = BrainMRIDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                          shuffle=True, collate_fn=collate_fn)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                          shuffle=False, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "tKS1lYTwamsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lets look at some images from the dataset"
      ],
      "metadata": {
        "id": "B9P784d3itbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_images =2\n",
        "plt.figure(figsize=(16, 14))\n",
        "\n",
        "for i in range(num_images):\n",
        "  image = X_train[i]\n",
        "  label = y_train[i]\n",
        "  text = text_prompts[label]\n",
        "\n",
        "  plt.subplot(1, num_images, i + 1)\n",
        "  plt.imshow(image)\n",
        "  plt.title(text, fontsize= 14)\n",
        "  plt.axis('off')\n",
        "  plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vkMbfDTyiM8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and evaluation loop"
      ],
      "metadata": {
        "id": "LGZrAGQfatcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_auc_history = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    clip_model.train()\n",
        "    train_loss = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        images = batch[\"images\"]\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        # Process batch with CLIP processor\n",
        "        inputs = clip_processor(\n",
        "            text=text_prompts,\n",
        "            images=images,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True\n",
        "        ).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = clip_model(**inputs)\n",
        "\n",
        "        logits = outputs.logits_per_image  # (batch, 2)\n",
        "        loss = loss_fn(logits, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "\n",
        "    # validation\n",
        "    clip_model.eval()\n",
        "    val_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            images = batch[\"images\"]\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            inputs = clip_processor(\n",
        "                text=text_prompts,\n",
        "                images=images,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True\n",
        "            ).to(device)\n",
        "\n",
        "            outputs = clip_model(**inputs)\n",
        "            logits = outputs.logits_per_image\n",
        "            loss = loss_fn(logits, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            probs = torch.softmax(logits, dim=1)[:, 1]  # probability of tumor\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "    val_loss /= len(test_loader)\n",
        "    val_acc = accuracy_score(all_labels, all_preds)\n",
        "    val_auc = roc_auc_score(all_labels, all_probs)\n",
        "    val_auc_history.append(val_auc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} \"\n",
        "          f\"- Train Loss: {train_loss} \"\n",
        "          f\"- Val Loss: {val_loss} \"\n",
        "          f\"- Val Acc: {val_acc} \"\n",
        "          f\"- Val AUC: {val_auc}\")\n",
        "\n",
        "#  Plot Validation AUC\n",
        "\n",
        "plt.plot(val_auc_history)\n",
        "plt.title(\"Validation AUC over Epochs\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"AUC\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IN9WW_ryavNk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}